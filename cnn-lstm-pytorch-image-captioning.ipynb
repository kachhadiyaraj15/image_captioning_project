{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:28.952646Z","iopub.status.busy":"2023-05-15T12:01:28.952077Z","iopub.status.idle":"2023-05-15T12:01:28.958801Z","shell.execute_reply":"2023-05-15T12:01:28.957484Z","shell.execute_reply.started":"2023-05-15T12:01:28.952530Z"},"trusted":true},"outputs":[],"source":["RESIZE = 356\n","CROP = 299"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:28.961925Z","iopub.status.busy":"2023-05-15T12:01:28.961281Z","iopub.status.idle":"2023-05-15T12:01:38.287035Z","shell.execute_reply":"2023-05-15T12:01:38.286115Z","shell.execute_reply.started":"2023-05-15T12:01:28.961888Z"},"trusted":true},"outputs":[],"source":["import os \n","import pandas as pd \n","import spacy \n","import torch\n","from torch.nn.utils.rnn import pad_sequence  \n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  \n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","import sys\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import statistics\n","import torchvision.models as models\n","\n","torch.backends.cudnn.benchmark = True\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:38.289354Z","iopub.status.busy":"2023-05-15T12:01:38.288727Z","iopub.status.idle":"2023-05-15T12:01:39.662527Z","shell.execute_reply":"2023-05-15T12:01:39.660504Z","shell.execute_reply.started":"2023-05-15T12:01:38.289315Z"},"trusted":true},"outputs":[],"source":["spacy_eng = spacy.load(\"en\")\n","\n","def getNumberOfParameter(model):\n","    print('Number of trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n","    print('Total params: ', sum(p.numel() for p in model.parameters()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gensim\n","\n","# Download the pre-trained Word2Vec model (example: Google News Word2Vec)\n","# You can replace the path with the location of your downloaded model\n","word2vec_model_url = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_url, binary=True)\n","\n","class Vocabulary:\n","    def __init__(self, word2vec_model, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.word2vec_model = word2vec_model\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold and word in self.word2vec_model:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.665585Z","iopub.status.busy":"2023-05-15T12:01:39.664929Z","iopub.status.idle":"2023-05-15T12:01:39.684461Z","shell.execute_reply":"2023-05-15T12:01:39.683494Z","shell.execute_reply.started":"2023-05-15T12:01:39.665543Z"},"trusted":true},"outputs":[],"source":["class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, choose_file, isTrain, transform=None, freq_threshold=2):\n","        self.root_dir = root_dir\n","        \n","        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n","        self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n","        \n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.df[\"caption\"].tolist())\n","        \n","        #self.df = self.df.groupby('image').first().reset_index()\n","        \n","        self.choose_df = pd.read_csv(choose_file, names=['image'])\n","        if isTrain:\n","            validation_df = pd.read_csv('/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr_8k.valImages.txt',\n","                                    names=['image'])\n","            self.choose_df = pd.concat([self.choose_df, validation_df]).reset_index()\n","            \n","        self.df = self.df.loc[self.df['image'].isin(self.choose_df['image'].values)].reset_index(drop=True)\n","        \n","        self.transform = transform\n","\n","        # Get img, caption columns\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        img = Image.new('RGB', (RESIZE,RESIZE))\n","        try:\n","            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","        except:\n","            print(img_id)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.690439Z","iopub.status.busy":"2023-05-15T12:01:39.690142Z","iopub.status.idle":"2023-05-15T12:01:39.701164Z","shell.execute_reply":"2023-05-15T12:01:39.700389Z","shell.execute_reply.started":"2023-05-15T12:01:39.690410Z"},"trusted":true},"outputs":[],"source":["class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","        \n","        return imgs, targets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.704143Z","iopub.status.busy":"2023-05-15T12:01:39.703787Z","iopub.status.idle":"2023-05-15T12:01:39.718933Z","shell.execute_reply":"2023-05-15T12:01:39.717978Z","shell.execute_reply.started":"2023-05-15T12:01:39.704115Z"},"trusted":true},"outputs":[],"source":["def get_loader(\n","    root_folder,\n","    annotation_file,\n","    choose_file,\n","    transform,\n","    batch_size=32,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    isTrain=True\n","):\n","    dataset = FlickrDataset(root_folder, annotation_file, choose_file, transform=transform, isTrain=isTrain)\n","\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","    loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=shuffle,\n","        pin_memory=pin_memory,\n","        collate_fn=MyCollate(pad_idx=pad_idx),\n","    )\n","\n","    return loader, dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.720914Z","iopub.status.busy":"2023-05-15T12:01:39.720471Z","iopub.status.idle":"2023-05-15T12:01:39.733615Z","shell.execute_reply":"2023-05-15T12:01:39.732527Z","shell.execute_reply.started":"2023-05-15T12:01:39.720872Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, train_CNN, encoded_image_size=14):\n","        super(EncoderCNN, self).__init__()\n","        self.train_CNN = train_CNN\n","        self.encoded_image_size = encoded_image_size\n","        \n","        resnet = models.resnet18(pretrained=True)\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","        \n","        # Fine tune\n","        for param in self.resnet.parameters():\n","            param.requires_grad = False\n","        \n","        # Only train last 2 layers of resnet if at all required\n","        if train_CNN:\n","            for c in list(self.resnet.children())[-2:]:\n","                for p in c.parameters():\n","                    p.requires_grad = trainCNN\n","        \n","\n","    def forward(self, images):\n","        features = self.resnet(images) \n","        features = self.adaptive_pool(features) # batch, 512, encoded_image_size, encoded_image_size\n","        features = features.permute(0, 2, 3, 1) # batch, encoded_image_size, encoded_image_size, 512\n","        return features\n","            \n","            "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.737215Z","iopub.status.busy":"2023-05-15T12:01:39.736944Z","iopub.status.idle":"2023-05-15T12:01:39.748049Z","shell.execute_reply":"2023-05-15T12:01:39.747355Z","shell.execute_reply.started":"2023-05-15T12:01:39.737183Z"},"trusted":true},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n","        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1) \n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        att1 = self.encoder_att(encoder_out)\n","        att2 = self.decoder_att(decoder_hidden)\n","        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)  # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n","\n","        return attention_weighted_encoding, alpha\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.751165Z","iopub.status.busy":"2023-05-15T12:01:39.750567Z","iopub.status.idle":"2023-05-15T12:01:39.771160Z","shell.execute_reply":"2023-05-15T12:01:39.770445Z","shell.execute_reply.started":"2023-05-15T12:01:39.751127Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout):\n","        \"\"\"\n","        decoder_dim is hidden_size for lstm cell\n","        \"\"\"\n","        super(DecoderRNN, self).__init__()\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","        \n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n","        \n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.dropout = nn.Dropout(p=self.dropout)\n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n","        self.init_weights()  # initialize some layers with the uniform distribution\n","        \n","    def init_weights(self):\n","        \"\"\"\n","        Initializes some parameters with values from the uniform distribution, for easier convergence.\n","        \"\"\"\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","    \n","    def init_hidden_state(self, encoder_out):\n","        \"\"\"\n","        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :return: hidden state, cell state\n","        \"\"\"\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions):\n","        \"\"\"\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n","        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n","        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n","        \"\"\"\n","        \n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size = self.vocab_size\n","        \n","        # Flatten image\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","        \n","        # Embedding\n","        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n","        \n","        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","        \n","        decode_length = encoded_captions.size(1)-1\n","        \n","        # Create tensors to hold word predicion scores and alphas\n","        predictions = torch.zeros(batch_size, decode_length, vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, decode_length, num_pixels).to(device)\n","        \n","        for t in range(decode_length):\n","            \n","            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n","            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","                        \n","            h, c = self.decode_step(torch.cat([embeddings[:, t, :], attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:, t, :] = preds\n","            alphas[:, t, :] = alpha\n","\n","        return predictions, alphas\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.773251Z","iopub.status.busy":"2023-05-15T12:01:39.772833Z","iopub.status.idle":"2023-05-15T12:01:39.786808Z","shell.execute_reply":"2023-05-15T12:01:39.785917Z","shell.execute_reply.started":"2023-05-15T12:01:39.773191Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.790514Z","iopub.status.busy":"2023-05-15T12:01:39.790217Z","iopub.status.idle":"2023-05-15T12:01:39.806594Z","shell.execute_reply":"2023-05-15T12:01:39.805588Z","shell.execute_reply.started":"2023-05-15T12:01:39.790473Z"},"trusted":true},"outputs":[],"source":["class CNNtoRNN(nn.Module):\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5, train_CNN=False):\n","        super(CNNtoRNN, self).__init__()\n","        self.encoderCNN = EncoderCNN(train_CNN=train_CNN)\n","        self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n","                                     encoder_dim=encoder_dim, dropout=dropout)\n","\n","    def forward(self, images, captions):\n","        encoder_out = self.encoderCNN(images)\n","        outputs, alphas = self.decoderRNN(encoder_out, captions)\n","        return outputs, alphas\n","\n","    def caption_image(self, image, vocabulary, max_length=50):\n","        result_caption = [1]\n","    \n","        with torch.no_grad():\n","            encoder_out = self.encoderCNN(image)\n","            \n","            batch_size = encoder_out.size(0)\n","            encoder_dim = encoder_out.size(-1)\n","            vocab_size = self.decoderRNN.vocab_size\n","            \n","            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","            num_pixels = encoder_out.size(1)\n","            \n","            # initially start with sos as a predicted word\n","            predicted = torch.tensor([vocabulary.stoi[\"<SOS>\"]]).to(device)\n","            h, c = self.decoderRNN.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","            \n","            for t in range(max_length):\n","                embeddings = self.decoderRNN.embedding(predicted)  # (1, embed_dim)\n","                \n","                attention_weighted_encoding, alpha = self.decoderRNN.attention(encoder_out, h)\n","                gate = self.decoderRNN.sigmoid(self.decoderRNN.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n","                attention_weighted_encoding = gate * attention_weighted_encoding\n","                \n","                h, c = self.decoderRNN.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n","                preds = self.decoderRNN.fc(self.decoderRNN.dropout(h))  # (batch_size_t, vocab_size)\n","                    \n","                predicted = preds.argmax(1)\n","                result_caption.append(predicted.item())\n","                \n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","            \n","            return [vocabulary.itos[idx] for idx in result_caption]\n","            "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-15T12:01:39.810537Z","iopub.status.busy":"2023-05-15T12:01:39.810227Z","iopub.status.idle":"2023-05-15T12:01:40.107595Z","shell.execute_reply":"2023-05-15T12:01:40.105238Z","shell.execute_reply.started":"2023-05-15T12:01:39.810479Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8c6ed77736a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0misTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-566081467b10>\u001b[0m in \u001b[0;36mget_loader\u001b[0;34m(root_folder, annotation_file, choose_file, transform, batch_size, num_workers, shuffle, pin_memory, isTrain)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0misTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ):\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlickrDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoose_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpad_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<PAD>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-af49a62aae5c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, captions_file, choose_file, isTrain, transform, freq_threshold)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt'"]}],"source":["# get the losses for vizualization\n","losses = list()\n","val_losses = list()\n","# Train the model\n","batch_size=8\n","transform = transforms.Compose(\n","    [\n","        transforms.Resize((RESIZE, RESIZE)),\n","        transforms.RandomCrop((CROP, CROP)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")\n","train_loader, dataset = get_loader(\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flicker8k_Images',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr_8k.trainImages.txt',\n","    transform=transform,\n","    num_workers=8,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    isTrain=True\n",")\n","\n","val_loader, val_dataset = get_loader(\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flicker8k_Images',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr_8k.valImages.txt',\n","    transform=transform,\n","    num_workers=8,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    isTrain=True\n",")\n","# Hyperparameters\n","attention_dim = 700\n","embed_dim = 700\n","decoder_dim = 700\n","dropout = 0.5\n","vocab_size = len(dataset.vocab)\n","learning_rate = 1e-03\n","num_epochs = 1\n","load_model = False\n","save_model = True\n","train_CNN = False\n","alpha_c = 1\n","\n","def train():\n","\n","    # for tensorboard\n","    #writer = SummaryWriter(\"runs/flickr\")\n","    step = 0\n","\n","    # initialize model, loss etc\n","    model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","    if load_model:\n","        #step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","        step = load_checkpoint(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\"), model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        \n","        loss = 400\n","        if save_model:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","                \"step\": step,\n","            }\n","            save_checkpoint(checkpoint)\n","            torch.save(model.state_dict(), 'puremodel.pth.tar')\n","\n","        for idx, (imgs, captions) in tqdm(\n","            enumerate(train_loader), total=len(train_loader), leave=False\n","        ):\n","\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs, alphas = model(imgs, captions.permute(1, 0))\n","            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n","\n","            #writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            \n","            # Add doubly stochastic attention regularization\n","            #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","            \n","            optimizer.step()\n","            \n","        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\\\n","        \n","        \n","        \n","        model.eval()\n","\n","        for epoch in range(num_epochs):\n","\n","            loss = 400\n","            if save_model:\n","                checkpoint = {\n","                    \"state_dict\": model.state_dict(),\n","                    \"optimizer\": optimizer.state_dict(),\n","                    \"step\": step,\n","                }\n","                save_checkpoint(checkpoint)\n","                torch.save(model.state_dict(), 'puremodel.pth.tar')\n","\n","            for idx, (imgs, captions) in tqdm(\n","                enumerate(val_loader), total=len(val_loader), leave=False\n","            ):\n","\n","                imgs = imgs.to(device)\n","                captions = captions.to(device)\n","\n","                outputs, alphas = model(imgs, captions.permute(1, 0))\n","                loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n","\n","                #writer.add_scalar(\"Validation loss\", loss.item(), global_step=step)\n","                step += 1\n","\n","                optimizer.zero_grad()\n","                loss.backward(loss)\n","                \n","                optimizer.step()\n","            \n","        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\n","    \n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.108926Z","iopub.status.idle":"2023-05-15T12:01:40.109801Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","model.eval()\n","loss = 400\n","\n","for idx, (imgs, captions) in tqdm(\n","    enumerate(val_loader), total=len(val_loader), leave=False\n","):\n","\n","    imgs = imgs.to(device)\n","    captions = captions.to(device)\n","\n","    outputs, alphas = model(imgs, captions.permute(1, 0))\n","    loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n","\n","print(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.111280Z","iopub.status.idle":"2023-05-15T12:01:40.112202Z"},"trusted":true},"outputs":[],"source":["loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.113522Z","iopub.status.idle":"2023-05-15T12:01:40.114366Z"},"trusted":true},"outputs":[],"source":["test_loader, test_dataset = get_loader(\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flicker8k_Images',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt',\n","    '/kaggle/input/d/anantmohan/flickr8k/Data/Flickr8k_text/Flickr_8k.testImages.txt',\n","    transform=transform,\n","    num_workers=8,\n","    shuffle=False,\n","    isTrain=False\n",")\n","\n","model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n","model.load_state_dict(torch.load(\"./my_checkpoint.pth.tar\")['state_dict'])\n","#model.load_state_dict(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\")['state_dict'])\n","model.eval()\n","\n","predicted_captions = []\n","i = 0\n","for idx, (imgs, captions) in tqdm(\n","            enumerate(test_loader), total=len(test_loader), leave=False\n","        ):\n","    for k in range(imgs.shape[0]):\n","        img = imgs[k].unsqueeze(0)\n","        real_caption = [dataset.vocab.itos[j.item()] for j in captions[:, k]]\n","        predicted_captions.append([model.caption_image(img.to(device), dataset.vocab), real_caption])\n","        i += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.115592Z","iopub.status.idle":"2023-05-15T12:01:40.116487Z"},"trusted":true},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.117796Z","iopub.status.idle":"2023-05-15T12:01:40.118668Z"},"trusted":true},"outputs":[],"source":["import nltk \n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.120115Z","iopub.status.idle":"2023-05-15T12:01:40.121000Z"},"trusted":true},"outputs":[],"source":["# from nltk.translate import meteor_score\n","\n","# list_real ="]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.122304Z","iopub.status.idle":"2023-05-15T12:01:40.123181Z"},"trusted":true},"outputs":[],"source":["i = 0\n","references_corpus = []\n","candidate_corpus = []\n","for e in predicted_captions:\n","    if i % 5 == 0:\n","        if i < 21:\n","            print('Image name: {}'.format(test_dataset.df['image'][i]))\n","            print('Real caption: ', [ e for e in predicted_captions[i][1] if e != '<PAD>'])\n","            print('Predicted caption: ', [ e for e in predicted_captions[i][0] if e != '<PAD>'])\n","            print('\\n')\n","        references_corpus.append([[ e for e in predicted_captions[i][1] if e != '<PAD>']])\n","        candidate_corpus.append([ e for e in predicted_captions[i][0] if e != '<PAD>'])\n","        \n","    else:\n","        references_corpus[i//5].append([ e for e in predicted_captions[i][1] if e != '<PAD>'])\n","        \n","    i+=1\n","\n","from torchtext.data.metrics import bleu_score\n","print(bleu_score(candidate_corpus, references_corpus))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.124457Z","iopub.status.idle":"2023-05-15T12:01:40.125328Z"},"trusted":true},"outputs":[],"source":["getNumberOfParameter(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.126672Z","iopub.status.idle":"2023-05-15T12:01:40.127627Z"},"trusted":true},"outputs":[],"source":["hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.128941Z","iopub.status.idle":"2023-05-15T12:01:40.129840Z"},"trusted":true},"outputs":[],"source":["reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.131225Z","iopub.status.idle":"2023-05-15T12:01:40.132110Z"},"trusted":true},"outputs":[],"source":["round(single_meteor_score(reference1, hypothesis1),4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:01:40.133350Z","iopub.status.idle":"2023-05-15T12:01:40.134288Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","base_path = '../input/flickr8k/subjective_img/subjective_img/'\n","def showAndCaptionImage(img, model):\n","    \n","    img = Image.open(base_path + img).convert(\"RGB\")\n","    plt.imshow(img)\n","    plt.show()\n","    img = transform(img)\n","    caption = model.caption_image(img.unsqueeze(0).to(device), dataset.vocab)[1:-1]\n","    captionStr = \"\"\n","    for e in caption:\n","        captionStr += e + \" \"\n","    print(captionStr)\n","\n","subjective_images = ['sample1.jpg','sample2.jpg','sample3.jpg','sample4.jpg','sample5.jpg']\n","for image in subjective_images:\n","    showAndCaptionImage(image, model)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
