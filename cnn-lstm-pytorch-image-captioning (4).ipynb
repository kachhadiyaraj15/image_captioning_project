{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport pandas as pd \nimport spacy \nimport torch\nfrom torch.nn.utils.rnn import pad_sequence  \nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  \nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport sys\nimport numpy as np\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport statistics\nimport torchvision.models as models\n\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:02.079286Z","iopub.execute_input":"2023-05-15T15:25:02.079848Z","iopub.status.idle":"2023-05-15T15:25:02.088146Z","shell.execute_reply.started":"2023-05-15T15:25:02.079806Z","shell.execute_reply":"2023-05-15T15:25:02.087185Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"spacy_eng = spacy.load(\"en_core_web_sm\")\n\ndef getNumberOfParameter(model):\n    print('Number of trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n    print('Total params: ', sum(p.numel() for p in model.parameters()))\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:02.090555Z","iopub.execute_input":"2023-05-15T15:25:02.091169Z","iopub.status.idle":"2023-05-15T15:25:03.059066Z","shell.execute_reply.started":"2023-05-15T15:25:02.091123Z","shell.execute_reply":"2023-05-15T15:25:03.058168Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, choose_file, isTrain, transform=None, freq_threshold=2):\n        self.root_dir = root_dir\n        \n        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n        self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n        \n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.df[\"caption\"].tolist())\n        \n        #self.df = self.df.groupby('image').first().reset_index()\n        \n        self.choose_df = pd.read_csv(choose_file, names=['image'])\n        if isTrain:\n            validation_df = pd.read_csv('/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n                                    names=['image'])\n            self.choose_df = pd.concat([self.choose_df, validation_df]).reset_index()\n            \n        self.df = self.df.loc[self.df['image'].isin(self.choose_df['image'].values)].reset_index(drop=True)\n        \n        self.transform = transform\n\n        # Get img, caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.new('RGB', (RESIZE,RESIZE))\n        try:\n            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n        except:\n            print(img_id)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.060748Z","iopub.execute_input":"2023-05-15T15:25:03.061132Z","iopub.status.idle":"2023-05-15T15:25:03.074613Z","shell.execute_reply.started":"2023-05-15T15:25:03.061093Z","shell.execute_reply":"2023-05-15T15:25:03.073344Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        \n        return imgs, targets","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.076168Z","iopub.execute_input":"2023-05-15T15:25:03.076766Z","iopub.status.idle":"2023-05-15T15:25:03.090165Z","shell.execute_reply.started":"2023-05-15T15:25:03.076726Z","shell.execute_reply":"2023-05-15T15:25:03.089322Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    choose_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n    isTrain=True\n):\n    dataset = FlickrDataset(root_folder, annotation_file, choose_file, transform=transform, isTrain=isTrain)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.094126Z","iopub.execute_input":"2023-05-15T15:25:03.094443Z","iopub.status.idle":"2023-05-15T15:25:03.107137Z","shell.execute_reply.started":"2023-05-15T15:25:03.094396Z","shell.execute_reply":"2023-05-15T15:25:03.106333Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self):\n\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n    def forward(self, images):\n        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n\n        return features\n#     def __init__(self, train_CNN, encoded_image_size=14):\n#         super(EncoderCNN, self).__init__()\n#         self.train_CNN = train_CNN\n#         self.encoded_image_size = encoded_image_size\n        \n#         resnet = models.resnet18(pretrained=True)\n#         modules = list(resnet.children())[:-2]\n#         self.resnet = nn.Sequential(*modules)\n#         self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        \n#         # Fine tune\n#         for param in self.resnet.parameters():\n#             param.requires_grad = False\n        \n#         # Only train last 2 layers of resnet if at all required\n#         if train_CNN:\n#             for c in list(self.resnet.children())[-2:]:\n#                 for p in c.parameters():\n#                     p.requires_grad = trainCNN\n        \n\n#     def forward(self, images):\n#         features = self.resnet(images) \n#         features = self.adaptive_pool(features) # batch, 512, encoded_image_size, encoded_image_size\n#         features = features.permute(0, 2, 3, 1) # batch, encoded_image_size, encoded_image_size, 512\n#         return features\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.109920Z","iopub.execute_input":"2023-05-15T15:25:03.110285Z","iopub.status.idle":"2023-05-15T15:25:03.119828Z","shell.execute_reply.started":"2023-05-15T15:25:03.110246Z","shell.execute_reply":"2023-05-15T15:25:03.118722Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1) \n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.121511Z","iopub.execute_input":"2023-05-15T15:25:03.121865Z","iopub.status.idle":"2023-05-15T15:25:03.134561Z","shell.execute_reply.started":"2023-05-15T15:25:03.121829Z","shell.execute_reply":"2023-05-15T15:25:03.133733Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout):\n        \"\"\"\n        decoder_dim is hidden_size for lstm cell\n        \"\"\"\n        super(DecoderRNN, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        \n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        \n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n    \n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        \n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        \n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        \n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        \n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        decode_length = encoded_captions.size(1)-1\n        \n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, decode_length, vocab_size).to(device)\n        alphas = torch.zeros(batch_size, decode_length, num_pixels).to(device)\n        \n        for t in range(decode_length):\n            \n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n                        \n            h, c = self.decode_step(torch.cat([embeddings[:, t, :], attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            alphas[:, t, :] = alpha\n\n        return predictions, alphas\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.136059Z","iopub.execute_input":"2023-05-15T15:25:03.136480Z","iopub.status.idle":"2023-05-15T15:25:03.155761Z","shell.execute_reply.started":"2023-05-15T15:25:03.136436Z","shell.execute_reply":"2023-05-15T15:25:03.154912Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.157083Z","iopub.execute_input":"2023-05-15T15:25:03.157678Z","iopub.status.idle":"2023-05-15T15:25:03.171552Z","shell.execute_reply.started":"2023-05-15T15:25:03.157639Z","shell.execute_reply":"2023-05-15T15:25:03.170887Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5, train_CNN=False):\n        super(CNNtoRNN, self).__init__()\n        self.encoderCNN = EncoderCNN(train_CNN=train_CNN)\n        self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n                                     encoder_dim=encoder_dim, dropout=dropout)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoderCNN(images)\n        outputs, alphas = self.decoderRNN(encoder_out, captions)\n        return outputs, alphas\n\n    def caption_image(self, image, vocabulary, max_length=50):\n        result_caption = [1]\n    \n        with torch.no_grad():\n            encoder_out = self.encoderCNN(image)\n            \n            batch_size = encoder_out.size(0)\n            encoder_dim = encoder_out.size(-1)\n            vocab_size = self.decoderRNN.vocab_size\n            \n            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n            num_pixels = encoder_out.size(1)\n            \n            # initially start with sos as a predicted word\n            predicted = torch.tensor([vocabulary.stoi[\"<SOS>\"]]).to(device)\n            h, c = self.decoderRNN.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n            \n            for t in range(max_length):\n                embeddings = self.decoderRNN.embedding(predicted)  # (1, embed_dim)\n                \n                attention_weighted_encoding, alpha = self.decoderRNN.attention(encoder_out, h)\n                gate = self.decoderRNN.sigmoid(self.decoderRNN.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n                attention_weighted_encoding = gate * attention_weighted_encoding\n                \n                h, c = self.decoderRNN.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n                preds = self.decoderRNN.fc(self.decoderRNN.dropout(h))  # (batch_size_t, vocab_size)\n                    \n                predicted = preds.argmax(1)\n                result_caption.append(predicted.item())\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n            \n            return [vocabulary.itos[idx] for idx in result_caption]\n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.172968Z","iopub.execute_input":"2023-05-15T15:25:03.173312Z","iopub.status.idle":"2023-05-15T15:25:03.188230Z","shell.execute_reply.started":"2023-05-15T15:25:03.173282Z","shell.execute_reply":"2023-05-15T15:25:03.187291Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"RESIZE = 356\nCROP = 299","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.189542Z","iopub.execute_input":"2023-05-15T15:25:03.190051Z","iopub.status.idle":"2023-05-15T15:25:03.203520Z","shell.execute_reply.started":"2023-05-15T15:25:03.190013Z","shell.execute_reply":"2023-05-15T15:25:03.202864Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# get the losses for vizualization\nlosses = list()\nval_losses = list()\n# Train the model\nbatch_size=32\ntransform = transforms.Compose(\n    [\n        transforms.Resize((RESIZE, RESIZE)),\n        transforms.RandomCrop((CROP, CROP)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_loader, dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n\nval_loader, val_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n# Hyperparameters\nattention_dim = 700\nembed_dim = 700\ndecoder_dim = 700\ndropout = 0.5\nvocab_size = len(dataset.vocab)\nlearning_rate = 1e-03\nnum_epochs = 10\nload_model = False\nsave_model = True\ntrain_CNN = False\nalpha_c = 1\n\ndef train():\n    \n    step = 0\n\n    # initialize model, loss etc\n    model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    model.train()\n\n    for epoch in range(num_epochs):\n        \n        loss = 400\n        if save_model:\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"step\": step,\n            }\n            save_checkpoint(checkpoint)\n            torch.save(model.state_dict(), 'puremodel.pth.tar')\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(train_loader), total=len(train_loader), leave=False\n        ):\n\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs, alphas = model(imgs, captions.permute(1, 0))\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n\n            #writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n            step += 1\n\n            optimizer.zero_grad()\n            loss.backward(loss)\n            \n            # Add doubly stochastic attention regularization\n            #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n            \n            optimizer.step()\n            \n        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\\\n        \n        \n        \n        model.eval()\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(val_loader), total=len(val_loader), leave=False\n        ):\n\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs, alphas = model(imgs, captions.permute(1, 0))\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n               \n            \n        print('Epoch val {} completed with loss {}'.format(epoch+1, loss))\n    \ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:03.207026Z","iopub.execute_input":"2023-05-15T15:25:03.207319Z","iopub.status.idle":"2023-05-15T15:25:07.197855Z","shell.execute_reply.started":"2023-05-15T15:25:03.207291Z","shell.execute_reply":"2023-05-15T15:25:07.196498Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-b2098bf92aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch val {} completed with loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-b2098bf92aeb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# initialize model, loss etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNtoRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<PAD>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-996a993fab49>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout, train_CNN)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNtoRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_CNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_CNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n\u001b[1;32m      6\u001b[0m                                      encoder_dim=encoder_dim, dropout=dropout)\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train_CNN'"],"ename":"TypeError","evalue":"__init__() got an unexpected keyword argument 'train_CNN'","output_type":"error"}]},{"cell_type":"code","source":"test_loader, test_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.testImages.txt',\n    transform=transform,\n    num_workers=8,\n    shuffle=False,\n    isTrain=False\n)\n\nmodel = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\nmodel.load_state_dict(torch.load(\"./my_checkpoint.pth.tar\")['state_dict'])\n#model.load_state_dict(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\")['state_dict'])\nmodel.eval()\n\npredicted_captions = []\ni = 0\nfor idx, (imgs, captions) in tqdm(\n            enumerate(test_loader), total=len(test_loader), leave=False\n        ):\n    for k in range(imgs.shape[0]):\n        img = imgs[k].unsqueeze(0)\n        real_caption = [dataset.vocab.itos[j.item()] for j in captions[:, k]]\n        predicted_captions.append([model.caption_image(img.to(device), dataset.vocab), real_caption])\n        i += 1\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.199044Z","iopub.status.idle":"2023-05-15T15:25:07.199850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.201074Z","iopub.status.idle":"2023-05-15T15:25:07.201911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import nltk \n# nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.203087Z","iopub.status.idle":"2023-05-15T15:25:07.203879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.translate import meteor_score\n\n# list_real =","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.205075Z","iopub.status.idle":"2023-05-15T15:25:07.205893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nreferences_corpus = []\ncandidate_corpus = []\nfor e in predicted_captions:\n    if i % 5 == 0:\n        if i < 21:\n            print('Image name: {}'.format(test_dataset.df['image'][i]))\n            print('Real caption: ', [ e for e in predicted_captions[i][1] if e != '<PAD>'])\n            print('Predicted caption: ', [ e for e in predicted_captions[i][0] if e != '<PAD>'])\n            print('\\n')\n        references_corpus.append([[ e for e in predicted_captions[i][1] if e != '<PAD>']])\n        candidate_corpus.append([ e for e in predicted_captions[i][0] if e != '<PAD>'])\n        \n    else:\n        references_corpus[i//5].append([ e for e in predicted_captions[i][1] if e != '<PAD>'])\n        \n    i+=1\n\nfrom torchtext.data.metrics import bleu_score\nprint(bleu_score(candidate_corpus, references_corpus))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.207224Z","iopub.status.idle":"2023-05-15T15:25:07.208054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getNumberOfParameter(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.209192Z","iopub.status.idle":"2023-05-15T15:25:07.209984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.211194Z","iopub.status.idle":"2023-05-15T15:25:07.212002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.213151Z","iopub.status.idle":"2023-05-15T15:25:07.213942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# round(single_meteor_score(reference1, hypothesis1),4)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.215092Z","iopub.status.idle":"2023-05-15T15:25:07.215886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nbase_path = '/kaggle/input/subjective-img/'\ndef showAndCaptionImage(img, model):\n    \n    img = Image.open(base_path + img).convert(\"RGB\")\n    plt.imshow(img)\n    plt.show()\n    img = transform(img)\n    caption = model.caption_image(img.unsqueeze(0).to(device), dataset.vocab)[1:-1]\n    captionStr = \"\"\n    for e in caption:\n        captionStr += e + \" \"\n    print(captionStr)\n\nsubjective_images = ['sample1.jpg','sample2.jpg','sample3.jpg','sample4.jpg','sample5.jpg']\nfor image in subjective_images:\n    showAndCaptionImage(image, model)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T15:25:07.217086Z","iopub.status.idle":"2023-05-15T15:25:07.217915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}