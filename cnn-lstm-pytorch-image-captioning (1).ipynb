{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport pandas as pd \nimport spacy \nimport torch\nfrom torch.nn.utils.rnn import pad_sequence  \nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  \nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport sys\nimport numpy as np\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport statistics\nimport torchvision.models as models\n\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:50.887043Z","iopub.execute_input":"2023-05-15T12:48:50.887478Z","iopub.status.idle":"2023-05-15T12:48:55.476208Z","shell.execute_reply.started":"2023-05-15T12:48:50.887392Z","shell.execute_reply":"2023-05-15T12:48:55.475129Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"spacy_eng = spacy.load(\"en\")\n\ndef getNumberOfParameter(model):\n    print('Number of trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n    print('Total params: ', sum(p.numel() for p in model.parameters()))\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:55.480354Z","iopub.execute_input":"2023-05-15T12:48:55.480652Z","iopub.status.idle":"2023-05-15T12:48:56.708511Z","shell.execute_reply.started":"2023-05-15T12:48:55.480624Z","shell.execute_reply":"2023-05-15T12:48:56.707618Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, choose_file, isTrain, transform=None, freq_threshold=2):\n        self.root_dir = root_dir\n        \n        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n        self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n        \n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.df[\"caption\"].tolist())\n        \n        #self.df = self.df.groupby('image').first().reset_index()\n        \n        self.choose_df = pd.read_csv(choose_file, names=['image'])\n        if isTrain:\n            validation_df = pd.read_csv('/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n                                    names=['image'])\n            self.choose_df = pd.concat([self.choose_df, validation_df]).reset_index()\n            \n        self.df = self.df.loc[self.df['image'].isin(self.choose_df['image'].values)].reset_index(drop=True)\n        \n        self.transform = transform\n\n        # Get img, caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.new('RGB', (RESIZE,RESIZE))\n        try:\n            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n        except:\n            print(img_id)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.709990Z","iopub.execute_input":"2023-05-15T12:48:56.710371Z","iopub.status.idle":"2023-05-15T12:48:56.724022Z","shell.execute_reply.started":"2023-05-15T12:48:56.710332Z","shell.execute_reply":"2023-05-15T12:48:56.722432Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        \n        return imgs, targets","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.725808Z","iopub.execute_input":"2023-05-15T12:48:56.726220Z","iopub.status.idle":"2023-05-15T12:48:56.737191Z","shell.execute_reply.started":"2023-05-15T12:48:56.726154Z","shell.execute_reply":"2023-05-15T12:48:56.736301Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    choose_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n    isTrain=True\n):\n    dataset = FlickrDataset(root_folder, annotation_file, choose_file, transform=transform, isTrain=isTrain)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.741045Z","iopub.execute_input":"2023-05-15T12:48:56.741358Z","iopub.status.idle":"2023-05-15T12:48:56.753024Z","shell.execute_reply.started":"2023-05-15T12:48:56.741327Z","shell.execute_reply":"2023-05-15T12:48:56.752074Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, train_CNN, encoded_image_size=14):\n        super(EncoderCNN, self).__init__()\n        self.train_CNN = train_CNN\n        self.encoded_image_size = encoded_image_size\n        \n        resnet = models.resnet18(pretrained=True)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        \n        # Fine tune\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n        \n        # Only train last 2 layers of resnet if at all required\n        if train_CNN:\n            for c in list(self.resnet.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = trainCNN\n        \n\n    def forward(self, images):\n        features = self.resnet(images) \n        features = self.adaptive_pool(features) # batch, 512, encoded_image_size, encoded_image_size\n        features = features.permute(0, 2, 3, 1) # batch, encoded_image_size, encoded_image_size, 512\n        return features\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.755448Z","iopub.execute_input":"2023-05-15T12:48:56.755810Z","iopub.status.idle":"2023-05-15T12:48:56.764708Z","shell.execute_reply.started":"2023-05-15T12:48:56.755774Z","shell.execute_reply":"2023-05-15T12:48:56.763829Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1) \n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.765911Z","iopub.execute_input":"2023-05-15T12:48:56.766441Z","iopub.status.idle":"2023-05-15T12:48:56.779856Z","shell.execute_reply.started":"2023-05-15T12:48:56.766397Z","shell.execute_reply":"2023-05-15T12:48:56.779048Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout):\n        \"\"\"\n        decoder_dim is hidden_size for lstm cell\n        \"\"\"\n        super(DecoderRNN, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        \n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        \n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n    \n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \"\"\"\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        \n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        \n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        \n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        \n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        decode_length = encoded_captions.size(1)-1\n        \n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, decode_length, vocab_size).to(device)\n        alphas = torch.zeros(batch_size, decode_length, num_pixels).to(device)\n        \n        for t in range(decode_length):\n            \n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n                        \n            h, c = self.decode_step(torch.cat([embeddings[:, t, :], attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            alphas[:, t, :] = alpha\n\n        return predictions, alphas\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.781479Z","iopub.execute_input":"2023-05-15T12:48:56.781982Z","iopub.status.idle":"2023-05-15T12:48:56.801526Z","shell.execute_reply.started":"2023-05-15T12:48:56.781946Z","shell.execute_reply":"2023-05-15T12:48:56.800670Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.802863Z","iopub.execute_input":"2023-05-15T12:48:56.803372Z","iopub.status.idle":"2023-05-15T12:48:56.816420Z","shell.execute_reply.started":"2023-05-15T12:48:56.803336Z","shell.execute_reply":"2023-05-15T12:48:56.815600Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5, train_CNN=False):\n        super(CNNtoRNN, self).__init__()\n        self.encoderCNN = EncoderCNN(train_CNN=train_CNN)\n        self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n                                     encoder_dim=encoder_dim, dropout=dropout)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoderCNN(images)\n        outputs, alphas = self.decoderRNN(encoder_out, captions)\n        return outputs, alphas\n\n    def caption_image(self, image, vocabulary, max_length=50):\n        result_caption = [1]\n    \n        with torch.no_grad():\n            encoder_out = self.encoderCNN(image)\n            \n            batch_size = encoder_out.size(0)\n            encoder_dim = encoder_out.size(-1)\n            vocab_size = self.decoderRNN.vocab_size\n            \n            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n            num_pixels = encoder_out.size(1)\n            \n            # initially start with sos as a predicted word\n            predicted = torch.tensor([vocabulary.stoi[\"<SOS>\"]]).to(device)\n            h, c = self.decoderRNN.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n            \n            for t in range(max_length):\n                embeddings = self.decoderRNN.embedding(predicted)  # (1, embed_dim)\n                \n                attention_weighted_encoding, alpha = self.decoderRNN.attention(encoder_out, h)\n                gate = self.decoderRNN.sigmoid(self.decoderRNN.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n                attention_weighted_encoding = gate * attention_weighted_encoding\n                \n                h, c = self.decoderRNN.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n                preds = self.decoderRNN.fc(self.decoderRNN.dropout(h))  # (batch_size_t, vocab_size)\n                    \n                predicted = preds.argmax(1)\n                result_caption.append(predicted.item())\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n            \n            return [vocabulary.itos[idx] for idx in result_caption]\n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.819546Z","iopub.execute_input":"2023-05-15T12:48:56.819815Z","iopub.status.idle":"2023-05-15T12:48:56.834799Z","shell.execute_reply.started":"2023-05-15T12:48:56.819790Z","shell.execute_reply":"2023-05-15T12:48:56.834105Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"RESIZE = 356\nCROP = 299","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:48:56.836215Z","iopub.execute_input":"2023-05-15T12:48:56.836687Z","iopub.status.idle":"2023-05-15T12:48:56.848180Z","shell.execute_reply.started":"2023-05-15T12:48:56.836651Z","shell.execute_reply":"2023-05-15T12:48:56.847444Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# get the losses for vizualization\nlosses = list()\nval_losses = list()\n# Train the model\nbatch_size=8\ntransform = transforms.Compose(\n    [\n        transforms.Resize((RESIZE, RESIZE)),\n        transforms.RandomCrop((CROP, CROP)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_loader, dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n\nval_loader, val_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n# Hyperparameters\nattention_dim = 700\nembed_dim = 700\ndecoder_dim = 700\ndropout = 0.5\nvocab_size = len(dataset.vocab)\nlearning_rate = 1e-03\nnum_epochs = 1\nload_model = False\nsave_model = True\ntrain_CNN = False\nalpha_c = 1\n\ndef train():\n    \n    step = 0\n\n    # initialize model, loss etc\n    model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    model.train()\n\n    for epoch in range(num_epochs):\n        \n        loss = 400\n        if save_model:\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"step\": step,\n            }\n            save_checkpoint(checkpoint)\n            torch.save(model.state_dict(), 'puremodel.pth.tar')\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(train_loader), total=len(train_loader), leave=False\n        ):\n\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs, alphas = model(imgs, captions.permute(1, 0))\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n\n            #writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n            step += 1\n\n            optimizer.zero_grad()\n            loss.backward(loss)\n            \n            # Add doubly stochastic attention regularization\n            #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n            \n            optimizer.step()\n            \n        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\\\n        \n        \n        \n        model.eval()\n\n        for epoch in range(num_epochs):\n\n            \n            for idx, (imgs, captions) in tqdm(\n                enumerate(val_loader), total=len(val_loader), leave=False\n            ):\n\n                imgs = imgs.to(device)\n                captions = captions.to(device)\n\n                outputs, alphas = model(imgs, captions.permute(1, 0))\n                loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n               \n            \n        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\n    \ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:51:19.122982Z","iopub.execute_input":"2023-05-15T12:51:19.123393Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 2093/4375 [03:28<03:36, 10.56it/s]","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\nmodel.eval()\nloss = 400\n\nfor idx, (imgs, captions) in tqdm(\n    enumerate(val_loader), total=len(val_loader), leave=False\n):\n\n    imgs = imgs.to(device)\n    captions = captions.to(device)\n\n    outputs, alphas = model(imgs, captions.permute(1, 0))\n    loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.750867Z","iopub.status.idle":"2023-05-15T12:50:00.751700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.752939Z","iopub.status.idle":"2023-05-15T12:50:00.753734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader, test_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.testImages.txt',\n    transform=transform,\n    num_workers=8,\n    shuffle=False,\n    isTrain=False\n)\n\nmodel = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\nmodel.load_state_dict(torch.load(\"./my_checkpoint.pth.tar\")['state_dict'])\n#model.load_state_dict(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\")['state_dict'])\nmodel.eval()\n\npredicted_captions = []\ni = 0\nfor idx, (imgs, captions) in tqdm(\n            enumerate(test_loader), total=len(test_loader), leave=False\n        ):\n    for k in range(imgs.shape[0]):\n        img = imgs[k].unsqueeze(0)\n        real_caption = [dataset.vocab.itos[j.item()] for j in captions[:, k]]\n        predicted_captions.append([model.caption_image(img.to(device), dataset.vocab), real_caption])\n        i += 1\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.754957Z","iopub.status.idle":"2023-05-15T12:50:00.755752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.756929Z","iopub.status.idle":"2023-05-15T12:50:00.757727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk \nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.758881Z","iopub.status.idle":"2023-05-15T12:50:00.759676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.translate import meteor_score\n\n# list_real =","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.760865Z","iopub.status.idle":"2023-05-15T12:50:00.761662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nreferences_corpus = []\ncandidate_corpus = []\nfor e in predicted_captions:\n    if i % 5 == 0:\n        if i < 21:\n            print('Image name: {}'.format(test_dataset.df['image'][i]))\n            print('Real caption: ', [ e for e in predicted_captions[i][1] if e != '<PAD>'])\n            print('Predicted caption: ', [ e for e in predicted_captions[i][0] if e != '<PAD>'])\n            print('\\n')\n        references_corpus.append([[ e for e in predicted_captions[i][1] if e != '<PAD>']])\n        candidate_corpus.append([ e for e in predicted_captions[i][0] if e != '<PAD>'])\n        \n    else:\n        references_corpus[i//5].append([ e for e in predicted_captions[i][1] if e != '<PAD>'])\n        \n    i+=1\n\nfrom torchtext.data.metrics import bleu_score\nprint(bleu_score(candidate_corpus, references_corpus))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.762858Z","iopub.status.idle":"2023-05-15T12:50:00.763652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getNumberOfParameter(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.764857Z","iopub.status.idle":"2023-05-15T12:50:00.765652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.766821Z","iopub.status.idle":"2023-05-15T12:50:00.767614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.768792Z","iopub.status.idle":"2023-05-15T12:50:00.769576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(single_meteor_score(reference1, hypothesis1),4)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.770707Z","iopub.status.idle":"2023-05-15T12:50:00.771542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nbase_path = '/kaggle/input/subjective-img'\ndef showAndCaptionImage(img, model):\n    \n    img = Image.open(base_path + img).convert(\"RGB\")\n    plt.imshow(img)\n    plt.show()\n    img = transform(img)\n    caption = model.caption_image(img.unsqueeze(0).to(device), dataset.vocab)[1:-1]\n    captionStr = \"\"\n    for e in caption:\n        captionStr += e + \" \"\n    print(captionStr)\n\nsubjective_images = ['sample1.jpg','sample2.jpg','sample3.jpg','sample4.jpg','sample5.jpg']\nfor image in subjective_images:\n    showAndCaptionImage(image, model)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:50:00.772726Z","iopub.status.idle":"2023-05-15T12:50:00.773511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}