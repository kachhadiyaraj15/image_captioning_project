{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport pandas as pd \nimport spacy \nimport torch\nfrom torch.nn.utils.rnn import pad_sequence  \nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image  \nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport sys\nimport numpy as np\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport statistics\nimport torchvision.models as models\n\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:22.768948Z","iopub.execute_input":"2023-05-15T19:05:22.769321Z","iopub.status.idle":"2023-05-15T19:05:44.785294Z","shell.execute_reply.started":"2023-05-15T19:05:22.769293Z","shell.execute_reply":"2023-05-15T19:05:44.784345Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import gensim\nword2vec_model_url = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_url, binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:44.787429Z","iopub.execute_input":"2023-05-15T19:05:44.788220Z","iopub.status.idle":"2023-05-15T19:05:44.793662Z","shell.execute_reply.started":"2023-05-15T19:05:44.788181Z","shell.execute_reply":"2023-05-15T19:05:44.792674Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, word2vec_model, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.word2vec_model = word2vec_model\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold and word in self.word2vec_model:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:44.795297Z","iopub.execute_input":"2023-05-15T19:05:44.795984Z","iopub.status.idle":"2023-05-15T19:05:44.811678Z","shell.execute_reply.started":"2023-05-15T19:05:44.795948Z","shell.execute_reply":"2023-05-15T19:05:44.810785Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, choose_file, isTrain, transform=None, freq_threshold=2):\n        self.root_dir = root_dir\n        \n        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n        self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n        \n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(word2vec_model,freq_threshold)\n        self.vocab.build_vocabulary(self.df[\"caption\"].tolist())\n        \n        #self.df = self.df.groupby('image').first().reset_index()\n        \n        self.choose_df = pd.read_csv(choose_file, names=['image'])\n        if isTrain:\n            validation_df = pd.read_csv('/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n                                    names=['image'])\n            self.choose_df = pd.concat([self.choose_df, validation_df]).reset_index()\n            \n        self.df = self.df.loc[self.df['image'].isin(self.choose_df['image'].values)].reset_index(drop=True)\n        \n        self.transform = transform\n\n        # Get img, caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img = Image.new('RGB', (RESIZE,RESIZE))\n        try:\n            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n        except:\n            print(img_id)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.283445Z","iopub.execute_input":"2023-05-15T19:05:46.283738Z","iopub.status.idle":"2023-05-15T19:05:46.296007Z","shell.execute_reply.started":"2023-05-15T19:05:46.283714Z","shell.execute_reply":"2023-05-15T19:05:46.295115Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        \n        return imgs, targets","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.297297Z","iopub.execute_input":"2023-05-15T19:05:46.298020Z","iopub.status.idle":"2023-05-15T19:05:46.321891Z","shell.execute_reply.started":"2023-05-15T19:05:46.297986Z","shell.execute_reply":"2023-05-15T19:05:46.320999Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    choose_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n    isTrain=True\n):\n    dataset = FlickrDataset(root_folder, annotation_file, choose_file, transform=transform, isTrain=isTrain)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.325010Z","iopub.execute_input":"2023-05-15T19:05:46.325636Z","iopub.status.idle":"2023-05-15T19:05:46.336997Z","shell.execute_reply.started":"2023-05-15T19:05:46.325603Z","shell.execute_reply":"2023-05-15T19:05:46.335965Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, train_CNN, encoded_image_size=14):\n        super(EncoderCNN, self).__init__()\n        self.train_CNN = train_CNN\n        self.encoded_image_size = encoded_image_size\n        \n        resnet = models.resnet18(pretrained=True)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        \n        # Fine tune\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n        \n        # Only train last 2 layers of resnet if at all required\n        if train_CNN:\n            for c in list(self.resnet.children())[-2:]:\n                for p in c.parameters():\n                    p.requires_grad = trainCNN\n        \n\n    def forward(self, images):\n        features = self.resnet(images) \n        features = self.adaptive_pool(features) # batch, 512, encoded_image_size, encoded_image_size\n        features = features.permute(0, 2, 3, 1) # batch, encoded_image_size, encoded_image_size, 512\n        return features\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.338408Z","iopub.execute_input":"2023-05-15T19:05:46.338928Z","iopub.status.idle":"2023-05-15T19:05:46.349357Z","shell.execute_reply.started":"2023-05-15T19:05:46.338897Z","shell.execute_reply":"2023-05-15T19:05:46.348450Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1) \n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.350950Z","iopub.execute_input":"2023-05-15T19:05:46.351280Z","iopub.status.idle":"2023-05-15T19:05:46.364697Z","shell.execute_reply.started":"2023-05-15T19:05:46.351250Z","shell.execute_reply":"2023-05-15T19:05:46.363694Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout):\n        \"\"\"\n        decoder_dim is hidden_size for lstm cell\n        \"\"\"\n        super(DecoderRNN, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        \n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        \n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions):\n        \n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        \n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        \n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        \n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        decode_length = encoded_captions.size(1)-1\n        \n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, decode_length, vocab_size).to(device)\n        alphas = torch.zeros(batch_size, decode_length, num_pixels).to(device)\n        \n        for t in range(decode_length):\n            \n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n                        \n            h, c = self.decode_step(torch.cat([embeddings[:, t, :], attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            alphas[:, t, :] = alpha\n\n        return predictions, alphas\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.369353Z","iopub.execute_input":"2023-05-15T19:05:46.369850Z","iopub.status.idle":"2023-05-15T19:05:46.388472Z","shell.execute_reply.started":"2023-05-15T19:05:46.369824Z","shell.execute_reply":"2023-05-15T19:05:46.387513Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.389814Z","iopub.execute_input":"2023-05-15T19:05:46.390695Z","iopub.status.idle":"2023-05-15T19:05:46.404577Z","shell.execute_reply.started":"2023-05-15T19:05:46.390660Z","shell.execute_reply":"2023-05-15T19:05:46.403512Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5, train_CNN=False):\n        super(CNNtoRNN, self).__init__()\n        self.encoderCNN = EncoderCNN(train_CNN=train_CNN)\n        self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n                                     encoder_dim=encoder_dim, dropout=dropout)\n\n    def forward(self, images, captions):\n        encoder_out = self.encoderCNN(images)\n        outputs, alphas = self.decoderRNN(encoder_out, captions)\n        return outputs, alphas\n\n    def caption_image(self, image, vocabulary, max_length=50):\n        result_caption = [1]\n    \n        with torch.no_grad():\n            encoder_out = self.encoderCNN(image)\n            \n            batch_size = encoder_out.size(0)\n            encoder_dim = encoder_out.size(-1)\n            vocab_size = self.decoderRNN.vocab_size\n            \n            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n            num_pixels = encoder_out.size(1)\n            \n            # initially start with sos as a predicted word\n            predicted = torch.tensor([vocabulary.stoi[\"<SOS>\"]]).to(device)\n            h, c = self.decoderRNN.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n            \n            for t in range(max_length):\n                embeddings = self.decoderRNN.embedding(predicted)  # (1, embed_dim)\n                \n                attention_weighted_encoding, alpha = self.decoderRNN.attention(encoder_out, h)\n                gate = self.decoderRNN.sigmoid(self.decoderRNN.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n                attention_weighted_encoding = gate * attention_weighted_encoding\n                \n                h, c = self.decoderRNN.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n                preds = self.decoderRNN.fc(self.decoderRNN.dropout(h))  # (batch_size_t, vocab_size)\n                    \n                predicted = preds.argmax(1)\n                result_caption.append(predicted.item())\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n            \n            return [vocabulary.itos[idx] for idx in result_caption]\n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.406287Z","iopub.execute_input":"2023-05-15T19:05:46.406710Z","iopub.status.idle":"2023-05-15T19:05:46.423857Z","shell.execute_reply.started":"2023-05-15T19:05:46.406678Z","shell.execute_reply":"2023-05-15T19:05:46.422766Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"RESIZE = 356\nCROP = 299","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.425379Z","iopub.execute_input":"2023-05-15T19:05:46.425741Z","iopub.status.idle":"2023-05-15T19:05:46.437580Z","shell.execute_reply.started":"2023-05-15T19:05:46.425710Z","shell.execute_reply":"2023-05-15T19:05:46.436509Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# get the losses for vizualization\nlosses = list()\nval_losses = list()\n# Train the model\nbatch_size=32\ntransform = transforms.Compose(\n    [\n        transforms.Resize((RESIZE, RESIZE)),\n        transforms.RandomCrop((CROP, CROP)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_loader, dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n\nval_loader, val_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt',\n    transform=transform,\n    num_workers=8,\n    batch_size=batch_size,\n    shuffle=True,\n    isTrain=True\n)\n# Hyperparameters\nattention_dim = 700\nembed_dim = 700\ndecoder_dim = 700\ndropout = 0.5\nvocab_size = len(dataset.vocab)\nlearning_rate = 2e-03\nnum_epochs = 3\nload_model = False\nsave_model = True\ntrain_CNN = False\nalpha_c = 1\n\ndef train():\n    \n    step = 0\n\n    # initialize model, loss etc\n    model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    model.train()\n\n    for epoch in range(num_epochs):\n        \n        loss = 400\n        if save_model:\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"step\": step,\n            }\n            save_checkpoint(checkpoint)\n            torch.save(model.state_dict(), 'puremodel.pth.tar')\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(train_loader), total=len(train_loader), leave=False\n        ):\n\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs, alphas = model(imgs, captions.permute(1, 0))\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n\n            #writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n            step += 1\n\n            optimizer.zero_grad()\n            loss.backward(loss)\n            \n            # Add doubly stochastic attention regularization\n            #loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n            \n            optimizer.step()\n            \n        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\\\n        \n        \n        \n        model.eval()\n\n        for idx, (imgs, captions) in tqdm(\n            enumerate(val_loader), total=len(val_loader), leave=False\n        ):\n\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n\n            outputs, alphas = model(imgs, captions.permute(1, 0))\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n               \n            \n        print('Epoch val {} completed with loss {}'.format(epoch+1, loss))\n    \ntrain()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:05:46.439992Z","iopub.execute_input":"2023-05-15T19:05:46.441214Z","iopub.status.idle":"2023-05-15T19:24:32.936773Z","shell.execute_reply.started":"2023-05-15T19:05:46.441181Z","shell.execute_reply":"2023-05-15T19:24:32.935709Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/3989402672.py:6: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/tmp/ipykernel_32/3989402672.py:6: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 200MB/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"                                                   ","output_type":"stream"},{"name":"stdout","text":"Epoch 1 completed with loss 2.9986822605133057\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch val 1 completed with loss 2.822374105453491\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"                                                   ","output_type":"stream"},{"name":"stdout","text":"Epoch 2 completed with loss 2.2355504035949707\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch val 2 completed with loss 2.273756504058838\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"                                                   ","output_type":"stream"},{"name":"stdout","text":"Epoch 3 completed with loss 2.281750202178955\n","output_type":"stream"},{"name":"stderr","text":"                                                 ","output_type":"stream"},{"name":"stdout","text":"Epoch val 3 completed with loss 1.9131556749343872\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"code","source":"test_loader, test_dataset = get_loader(\n    '/kaggle/input/flikerdata/Flickr8K/Flicker8k_Images',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr8k.token.txt',\n    '/kaggle/input/flikerdata/Flickr8K/Flickr8k_text/Flickr_8k.testImages.txt',\n    transform=transform,\n    num_workers=8,\n    shuffle=False,\n    isTrain=False\n)\n\nmodel = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\nmodel.load_state_dict(torch.load(\"./my_checkpoint.pth.tar\")['state_dict'])\n#model.load_state_dict(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\")['state_dict'])\nmodel.eval()\n\npredicted_captions = []\ni = 0\nfor idx, (imgs, captions) in tqdm(\n            enumerate(test_loader), total=len(test_loader), leave=False\n        ):\n    for k in range(imgs.shape[0]):\n        img = imgs[k].unsqueeze(0)\n        real_caption = [dataset.vocab.itos[j.item()] for j in captions[:, k]]\n        predicted_captions.append([model.caption_image(img.to(device), dataset.vocab), real_caption])\n        i += 1\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:24:32.939272Z","iopub.execute_input":"2023-05-15T19:24:32.940057Z","iopub.status.idle":"2023-05-15T19:26:07.086059Z","shell.execute_reply.started":"2023-05-15T19:24:32.940013Z","shell.execute_reply":"2023-05-15T19:26:07.084915Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/3989402672.py:6: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n  self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n                                                 \r","output_type":"stream"}]},{"cell_type":"code","source":"i = 0\nreferences_corpus = []\ncandidate_corpus = []\nfor e in predicted_captions:\n    if i % 5 == 0:\n        if i < 21:\n            print('Image name: {}'.format(test_dataset.df['image'][i]))\n            print('Real caption: ', [ e for e in predicted_captions[i][1] if e != '<PAD>'])\n            print('Predicted caption: ', [ e for e in predicted_captions[i][0] if e != '<PAD>'])\n            print('\\n')\n        references_corpus.append([[ e for e in predicted_captions[i][1] if e != '<PAD>']])\n        candidate_corpus.append([ e for e in predicted_captions[i][0] if e != '<PAD>'])\n        \n    else:\n        references_corpus[i//5].append([ e for e in predicted_captions[i][1] if e != '<PAD>'])\n        \n    i+=1\n\nfrom torchtext.data.metrics import bleu_score\nprint(bleu_score(candidate_corpus, references_corpus))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:26:07.118983Z","iopub.execute_input":"2023-05-15T19:26:07.119840Z","iopub.status.idle":"2023-05-15T19:26:09.884913Z","shell.execute_reply.started":"2023-05-15T19:26:07.119805Z","shell.execute_reply":"2023-05-15T19:26:09.883782Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Image name: 1056338697_4f7d7ce270.jpg\nReal caption:  ['<SOS>', 'a', 'blond', 'woman', 'in', 'a', 'blue', 'shirt', 'appears', 'to', 'wait', 'for', 'a', 'ride', '.', '<EOS>']\nPredicted caption:  ['<SOS>', 'a', 'woman', 'in', 'a', 'purple', 'shirt', 'is', 'sitting', 'on', 'a', 'bench', '.', '<EOS>']\n\n\nImage name: 106490881_5a2dd9b7bd.jpg\nReal caption:  ['<SOS>', 'a', 'boy', 'in', 'his', 'blue', 'swim', 'shorts', 'at', 'the', 'beach', '.', '<EOS>']\nPredicted caption:  ['<SOS>', 'a', 'young', 'boy', 'in', 'a', 'red', 'shirt', 'and', 'a', 'white', 'shirt', 'is', 'running', 'in', 'the', 'water', '.', '<EOS>']\n\n\nImage name: 1082379191_ec1e53f996.jpg\nReal caption:  ['<SOS>', 'a', 'lady', 'and', 'a', 'man', 'with', 'no', 'shirt', 'sit', 'on', 'a', 'dock', '.', '<EOS>']\nPredicted caption:  ['<SOS>', 'a', 'young', 'boy', 'is', 'jumping', 'into', 'a', 'pool', '.', '<EOS>']\n\n\nImage name: 1084040636_97d9633581.jpg\nReal caption:  ['<SOS>', 'a', 'closeup', 'of', 'a', 'white', 'dog', 'that', 'is', 'laying', 'its', 'head', 'on', 'its', 'paws', '.', '<EOS>']\nPredicted caption:  ['<SOS>', 'a', 'white', 'dog', 'is', 'running', 'on', 'a', 'beach', '.', '<EOS>']\n\n\nImage name: 1096395242_fc69f0ae5a.jpg\nReal caption:  ['<SOS>', 'a', 'boy', 'with', 'a', 'toy', 'gun', '.', '<EOS>']\nPredicted caption:  ['<SOS>', 'a', 'young', 'boy', 'is', 'playing', 'in', 'a', 'pile', 'of', 'water', '.', '<EOS>']\n\n\n0.21917486190795898\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}